{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPyR Tools - Advanced EPR Analysis\n",
    "\n",
    "This notebook demonstrates advanced analysis techniques for EPR spectroscopy using EPyR Tools.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Peak detection and fitting in EPR spectra\n",
    "- g-factor calculations and magnetic field calibration\n",
    "- Spectral integration for quantitative analysis\n",
    "- Double integration techniques\n",
    "- Hyperfine structure analysis\n",
    "- Spectrum simulation and comparison\n",
    "\n",
    "## Background\n",
    "\n",
    "Advanced EPR analysis involves extracting quantitative information from spectra:\n",
    "- **g-factors**: Electronic environment and orbital contributions\n",
    "- **Hyperfine coupling**: Nuclear-electron interactions\n",
    "- **Linewidths**: Relaxation processes and dynamics\n",
    "- **Concentrations**: Spin quantification through integration\n",
    "- **Anisotropy**: Orientational effects in powder or single crystal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize, integrate, signal\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Add EPyR Tools to path\n",
    "epyr_root = Path().resolve().parent.parent\n",
    "if str(epyr_root) not in sys.path:\n",
    "    sys.path.insert(0, str(epyr_root))\n",
    "\n",
    "import epyr.eprload as eprload\n",
    "from epyr.baseline import baseline_polynomial\n",
    "from epyr.constants import CONSTANTS\n",
    "import epyr.plot as epr_plot\n",
    "\n",
    "# Suppress minor warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"EPyR Tools advanced analysis module loaded successfully!\")\n",
    "print(f\"Physical constants available: {list(CONSTANTS.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare EPR Data\n",
    "\n",
    "We'll start with baseline-corrected data from the previous tutorial or load fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load previously corrected data first\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "corrected_files = list(processed_dir.glob(\"*baseline_corrected*.csv\"))\n",
    "\n",
    "if corrected_files:\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(f\"Found {len(corrected_files)} baseline-corrected files:\")\n",
    "    for f in corrected_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    # Load the first corrected file\n",
    "    df = pd.read_csv(corrected_files[0], comment='#')\n",
    "    x = df['field_gauss'].values\n",
    "    y_original = df['original_intensity'].values\n",
    "    y = df['corrected_intensity'].values\n",
    "    \n",
    "    print(f\"\\nLoaded corrected data: {len(x)} points\")\n",
    "    data_source = \"corrected\"\n",
    "    \n",
    "    # Estimate microwave frequency from g-factor assumption\n",
    "    center_field = x[np.argmax(np.abs(y))]\n",
    "    freq_estimate = CONSTANTS['g_free'] * CONSTANTS['mu_B'] * center_field * 1e-4 / CONSTANTS['h']\n",
    "    freq_ghz = freq_estimate / 1e9\n",
    "    \n",
    "else:\n",
    "    # Load fresh EPR data and apply baseline correction\n",
    "    data_dir = Path(\"../data\")\n",
    "    sample_file = None\n",
    "    \n",
    "    # Look for sample files\n",
    "    for file_path in (data_dir / \"BES3T\").glob(\"*.dsc\"):\n",
    "        sample_file = file_path\n",
    "        break\n",
    "    \n",
    "    if not sample_file:\n",
    "        for file_path in (data_dir / \"ESP\").glob(\"*.par\"):\n",
    "            sample_file = file_path\n",
    "            break\n",
    "    \n",
    "    if sample_file:\n",
    "        print(f\"Loading fresh EPR data: {sample_file.name}\")\n",
    "        x, y_original, params, filepath = eprload.eprload(str(sample_file), plot_if_possible=False)\n",
    "        \n",
    "        if x is not None and y_original is not None:\n",
    "            # Apply baseline correction\n",
    "            y, baseline_fit = baseline_polynomial(y_original, x_data=x, poly_order=2)\n",
    "            freq_ghz = params.get('MWFQ', params.get('MF', 9.4e9))\n",
    "            if isinstance(freq_ghz, str):\n",
    "                freq_ghz = float(freq_ghz)\n",
    "            freq_ghz = freq_ghz / 1e9 if freq_ghz > 1e6 else freq_ghz\n",
    "            data_source = \"fresh\"\n",
    "            print(f\"Applied baseline correction to {len(x)} points\")\n",
    "        else:\n",
    "            sample_file = None\n",
    "    \n",
    "    if sample_file is None:\n",
    "        # Create synthetic nitroxide spectrum for analysis demonstration\n",
    "        print(\"Creating synthetic nitroxide EPR spectrum for analysis...\")\n",
    "        \n",
    "        x = np.linspace(3300, 3400, 2000)  # High resolution for analysis\n",
    "        freq_ghz = 9.4  # X-band frequency\n",
    "        \n",
    "        # Nitroxide parameters (typical TEMPO)\n",
    "        g_iso = 2.006\n",
    "        A_N = 16.0  # Gauss, nitrogen hyperfine\n",
    "        linewidth = 1.2  # Gauss, intrinsic linewidth\n",
    "        \n",
    "        # Calculate field positions for three nitrogen lines\n",
    "        center_field = freq_ghz * 1e9 * CONSTANTS['h'] / (g_iso * CONSTANTS['mu_B']) * 1e4\n",
    "        field_positions = [center_field - A_N, center_field, center_field + A_N]\n",
    "        intensities = [1.0, 1.0, 1.0]  # Equal intensities for I=1 nitrogen\n",
    "        \n",
    "        # Generate spectrum as sum of Lorentzian lines\n",
    "        y = np.zeros_like(x)\n",
    "        for pos, intensity in zip(field_positions, intensities):\n",
    "            # Lorentzian lineshape (more realistic for solution EPR)\n",
    "            y += intensity * (linewidth/2)**2 / ((x - pos)**2 + (linewidth/2)**2)\n",
    "        \n",
    "        # Convert to first derivative (typical EPR display)\n",
    "        y = np.gradient(y, x)\n",
    "        \n",
    "        # Add realistic noise\n",
    "        noise_level = np.max(np.abs(y)) * 0.02\n",
    "        y += np.random.normal(0, noise_level, len(x))\n",
    "        \n",
    "        y_original = y.copy()  # No baseline issues in synthetic data\n",
    "        data_source = \"synthetic\"\n",
    "        \n",
    "        print(f\"Created synthetic nitroxide spectrum:\")\n",
    "        print(f\"  Frequency: {freq_ghz:.1f} GHz\")\n",
    "        print(f\"  g-factor: {g_iso:.3f}\")\n",
    "        print(f\"  A_N: {A_N:.1f} G\")\n",
    "        print(f\"  Linewidth: {linewidth:.1f} G\")\n",
    "\n",
    "print(f\"\\nData ready for analysis: {data_source} spectrum with {len(x)} points\")\n",
    "print(f\"Field range: {x.min():.1f} - {x.max():.1f} G\")\n",
    "print(f\"Microwave frequency: {freq_ghz:.3f} GHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data for analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "if 'y_original' in locals():\n",
    "    plt.plot(x, y_original, 'lightgray', linewidth=1, alpha=0.7, label='Original')\n",
    "plt.plot(x, y, 'b-', linewidth=1.5, label='EPR Spectrum (for analysis)')\n",
    "plt.xlabel('Magnetic Field (G)')\n",
    "plt.ylabel('EPR Signal (a.u.)')\n",
    "plt.title(f'EPR Spectrum Ready for Analysis - {freq_ghz:.1f} GHz')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show expanded view of main features\n",
    "plt.subplot(2, 1, 2)\n",
    "# Find the main signal region\n",
    "signal_std = np.std(y)\n",
    "signal_region = np.where(np.abs(y) > signal_std * 0.5)[0]\n",
    "\n",
    "if len(signal_region) > 0:\n",
    "    x_min, x_max = x[signal_region[0]], x[signal_region[-1]]\n",
    "    buffer = (x_max - x_min) * 0.2\n",
    "    x_zoom = (x >= x_min - buffer) & (x <= x_max + buffer)\n",
    "    \n",
    "    plt.plot(x[x_zoom], y[x_zoom], 'b-', linewidth=2, label='Main signal region')\n",
    "    plt.xlabel('Magnetic Field (G)')\n",
    "    plt.ylabel('EPR Signal (a.u.)')\n",
    "    plt.title('Expanded View of Main EPR Features')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basic spectral statistics\n",
    "print(\"\\nSpectral Analysis Overview:\")\n",
    "print(f\"Signal range: {np.min(y):.3f} to {np.max(y):.3f}\")\n",
    "print(f\"Peak-to-peak: {np.ptp(y):.3f}\")\n",
    "print(f\"RMS amplitude: {np.sqrt(np.mean(y**2)):.3f}\")\n",
    "print(f\"Signal-to-noise ratio: {np.ptp(y) / np.std(y[signal_region]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Peak Detection and Analysis\n",
    "\n",
    "Let's identify and analyze the main features in the EPR spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak detection in EPR spectra\n",
    "print(\"Detecting EPR spectral features...\")\n",
    "\n",
    "# For first-derivative EPR spectra, we look for zero-crossings and extrema\n",
    "def find_epr_peaks(x, y, threshold_factor=0.1):\n",
    "    \"\"\"Find peaks in EPR first-derivative spectrum.\"\"\"\n",
    "    \n",
    "    # Find zero crossings (absorption peak centers)\n",
    "    zero_crossings = []\n",
    "    for i in range(1, len(y)):\n",
    "        if y[i-1] * y[i] < 0:  # Sign change\n",
    "            # Linear interpolation to find exact crossing\n",
    "            x_cross = x[i-1] - y[i-1] * (x[i] - x[i-1]) / (y[i] - y[i-1])\n",
    "            zero_crossings.append(x_cross)\n",
    "    \n",
    "    # Find extrema (derivative peaks)\n",
    "    threshold = np.max(np.abs(y)) * threshold_factor\n",
    "    \n",
    "    # Positive peaks (upward parts of derivative)\n",
    "    peaks_pos, _ = signal.find_peaks(y, height=threshold, distance=5)\n",
    "    \n",
    "    # Negative peaks (downward parts of derivative)\n",
    "    peaks_neg, _ = signal.find_peaks(-y, height=threshold, distance=5)\n",
    "    \n",
    "    return zero_crossings, peaks_pos, peaks_neg\n",
    "\n",
    "zero_crossings, peaks_pos, peaks_neg = find_epr_peaks(x, y)\n",
    "\n",
    "print(f\"Found {len(zero_crossings)} absorption peaks (zero crossings)\")\n",
    "print(f\"Found {len(peaks_pos)} positive derivative peaks\")\n",
    "print(f\"Found {len(peaks_neg)} negative derivative peaks\")\n",
    "\n",
    "# Display peak positions\n",
    "if zero_crossings:\n",
    "    print(\"\\nAbsorption peak positions (G):\")\n",
    "    for i, pos in enumerate(zero_crossings):\n",
    "        print(f\"  Peak {i+1}: {pos:.2f} G\")\n",
    "        \n",
    "    # Calculate splittings if multiple peaks\n",
    "    if len(zero_crossings) > 1:\n",
    "        splittings = np.diff(zero_crossings)\n",
    "        print(f\"\\nHyperfine splittings (G):\")\n",
    "        for i, split in enumerate(splittings):\n",
    "            print(f\"  Δ{i+1}-{i+2}: {split:.2f} G\")\n",
    "        \n",
    "        if len(splittings) > 1:\n",
    "            print(f\"  Average splitting: {np.mean(splittings):.2f} ± {np.std(splittings):.2f} G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize peak detection results\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Main spectrum with detected features\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=1.5, label='EPR spectrum')\n",
    "\n",
    "# Mark zero crossings (absorption peaks)\n",
    "if zero_crossings:\n",
    "    for i, pos in enumerate(zero_crossings):\n",
    "        plt.axvline(x=pos, color='red', linestyle='--', alpha=0.7)\n",
    "        plt.text(pos, plt.ylim()[1]*0.9, f'Peak {i+1}\\n{pos:.1f} G', \n",
    "                ha='center', va='top', fontsize=9,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Mark derivative extrema\n",
    "if len(peaks_pos) > 0:\n",
    "    plt.plot(x[peaks_pos], y[peaks_pos], 'ro', markersize=6, label='Positive peaks')\n",
    "if len(peaks_neg) > 0:\n",
    "    plt.plot(x[peaks_neg], y[peaks_neg], 'go', markersize=6, label='Negative peaks')\n",
    "\n",
    "plt.xlabel('Magnetic Field (G)')\n",
    "plt.ylabel('EPR Signal (a.u.)')\n",
    "plt.title('EPR Peak Detection Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed view if there are multiple peaks\n",
    "plt.subplot(2, 1, 2)\n",
    "if zero_crossings and len(zero_crossings) > 1:\n",
    "    # Focus on region containing all peaks\n",
    "    peak_min, peak_max = min(zero_crossings), max(zero_crossings)\n",
    "    buffer = (peak_max - peak_min) * 0.3\n",
    "    zoom_mask = (x >= peak_min - buffer) & (x <= peak_max + buffer)\n",
    "    \n",
    "    plt.plot(x[zoom_mask], y[zoom_mask], 'b-', linewidth=2, label='EPR spectrum')\n",
    "    \n",
    "    # Mark features in zoomed view\n",
    "    for i, pos in enumerate(zero_crossings):\n",
    "        plt.axvline(x=pos, color='red', linestyle='--', alpha=0.7)\n",
    "        plt.text(pos, plt.ylim()[1]*0.8, f'{pos:.1f}', ha='center', va='top', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Magnetic Field (G)')\n",
    "    plt.ylabel('EPR Signal (a.u.)')\n",
    "    plt.title('Expanded View - Peak Positions and Splittings')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "else:\n",
    "    # Single peak analysis\n",
    "    plt.plot(x, np.cumsum(y) * (x[1] - x[0]), 'g-', linewidth=2, label='Integrated spectrum')\n",
    "    plt.xlabel('Magnetic Field (G)')\n",
    "    plt.ylabel('Integrated Intensity (a.u.)')\n",
    "    plt.title('Single Peak - Integration Profile')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis summary\n",
    "print(\"\\nPeak Analysis Summary:\")\n",
    "print(\"=\" * 30)\n",
    "if len(zero_crossings) == 1:\n",
    "    print(\"Single absorption peak detected - likely organic radical or transition metal\")\n",
    "elif len(zero_crossings) == 3:\n",
    "    print(\"Three-line pattern detected - likely nitroxide radical (I=1 nitrogen)\")\n",
    "elif len(zero_crossings) == 2:\n",
    "    print(\"Two-line pattern detected - possible doublet splitting\")\n",
    "elif len(zero_crossings) > 3:\n",
    "    print(f\"{len(zero_crossings)}-line pattern detected - complex hyperfine structure\")\n",
    "else:\n",
    "    print(\"No clear peaks detected - may need different analysis approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. g-Factor Calculations\n",
    "\n",
    "The g-factor is a fundamental parameter in EPR that provides information about the electronic environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate g-factors for detected peaks\n",
    "print(\"Calculating g-factors...\")\n",
    "\n",
    "def calculate_g_factor(field_gauss, frequency_hz):\n",
    "    \"\"\"Calculate g-factor from field and frequency.\"\"\"\n",
    "    g = CONSTANTS['h'] * frequency_hz / (CONSTANTS['mu_B'] * field_gauss * 1e-4)\n",
    "    return g\n",
    "\n",
    "freq_hz = freq_ghz * 1e9\n",
    "\n",
    "print(f\"Microwave frequency: {freq_ghz:.6f} GHz ({freq_hz:.0f} Hz)\")\n",
    "print(f\"Free electron g-factor: {CONSTANTS['g_free']:.6f}\")\n",
    "print()\n",
    "\n",
    "g_factors = []\n",
    "if zero_crossings:\n",
    "    print(\"Peak positions and g-factors:\")\n",
    "    print(f\"{'Peak':<8} {'Field (G)':<12} {'g-factor':<12} {'Δg':<12}\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    for i, field in enumerate(zero_crossings):\n",
    "        g = calculate_g_factor(field, freq_hz)\n",
    "        delta_g = g - CONSTANTS['g_free']\n",
    "        g_factors.append(g)\n",
    "        \n",
    "        print(f\"{i+1:<8} {field:<12.3f} {g:<12.6f} {delta_g:<+12.6f}\")\n",
    "    \n",
    "    # Statistical analysis of g-factors\n",
    "    if len(g_factors) > 1:\n",
    "        g_mean = np.mean(g_factors)\n",
    "        g_std = np.std(g_factors)\n",
    "        g_range = np.ptp(g_factors)\n",
    "        \n",
    "        print(f\"\\nStatistical Analysis:\")\n",
    "        print(f\"Mean g-factor: {g_mean:.6f} ± {g_std:.6f}\")\n",
    "        print(f\"g-factor range: {g_range:.6f}\")\n",
    "        print(f\"g-anisotropy: {g_range/g_mean*100:.3f}%\")\n",
    "        \n",
    "        # Check if isotropic (small g-spread) or anisotropic\n",
    "        if g_range < 0.001:\n",
    "            print(\"→ Isotropic system (solution or narrow powder pattern)\")\n",
    "        elif g_range < 0.01:\n",
    "            print(\"→ Weakly anisotropic system\")\n",
    "        else:\n",
    "            print(\"→ Strongly anisotropic system (powder pattern)\")\n",
    "    \n",
    "    else:\n",
    "        g = g_factors[0]\n",
    "        delta_g = g - CONSTANTS['g_free']\n",
    "        print(f\"\\nSingle peak analysis:\")\n",
    "        print(f\"g-factor: {g:.6f} (Δg = {delta_g:+.6f})\")\n",
    "        \n",
    "        # Interpret g-value\n",
    "        if abs(delta_g) < 0.0005:\n",
    "            print(\"→ Near free electron g-value: organic radical\")\n",
    "        elif delta_g > 0.002:\n",
    "            print(\"→ g > g_e: possible transition metal or heteroatom\")\n",
    "        elif delta_g < -0.002:\n",
    "            print(\"→ g < g_e: unusual, check frequency calibration\")\n",
    "        else:\n",
    "            print(\"→ Slightly shifted g-value: organic radical with some heterogeneity\")\n",
    "\n",
    "else:\n",
    "    print(\"No peaks detected for g-factor calculation\")\n",
    "    # Try to estimate from spectrum center\n",
    "    center_idx = len(x) // 2\n",
    "    center_field = x[center_idx]\n",
    "    g_estimate = calculate_g_factor(center_field, freq_hz)\n",
    "    print(f\"Estimated g-factor from spectrum center: {g_estimate:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced g-factor analysis and calibration\n",
    "print(\"\\nAdvanced g-Factor Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Field calibration using known standards\n",
    "print(\"\\n1. Field Calibration Check:\")\n",
    "known_standards = {\n",
    "    'DPPH': {'g': 2.0036, 'name': 'DPPH (organic radical)'},\n",
    "    'TEMPO': {'g': 2.0061, 'name': 'TEMPO (nitroxide)'},\n",
    "    'Mn2+': {'g': 2.001, 'name': 'Mn²⁺ in MgO'},\n",
    "    'E_center': {'g': 2.0006, 'name': 'E\\' center in quartz'}\n",
    "}\n",
    "\n",
    "if g_factors:\n",
    "    measured_g = np.mean(g_factors)\n",
    "    \n",
    "    print(f\"Measured g-factor: {measured_g:.6f}\")\n",
    "    print(\"Comparison with known standards:\")\n",
    "    \n",
    "    for std_name, std_data in known_standards.items():\n",
    "        delta = abs(measured_g - std_data['g'])\n",
    "        match = \"★\" if delta < 0.001 else \"\" if delta < 0.005 else \"\"\n",
    "        print(f\"  {std_data['name']:<25}: g = {std_data['g']:.6f}, Δ = {delta:.6f} {match}\")\n",
    "\n",
    "# Temperature effects on g-factor\n",
    "print(\"\\n2. Expected g-Factor Variations:\")\n",
    "print(\"Temperature effects: Δg ≈ ±0.0001 per 100K\")\n",
    "print(\"Field drift: Δg ≈ ±0.0002 typical\")\n",
    "print(\"Frequency stability: Δg ≈ ±0.00005 for good sources\")\n",
    "\n",
    "# Precision assessment\n",
    "if len(g_factors) > 1:\n",
    "    g_precision = np.std(g_factors)\n",
    "    print(f\"\\n3. Measurement Precision:\")\n",
    "    print(f\"g-factor standard deviation: {g_precision:.6f}\")\n",
    "    \n",
    "    if g_precision < 0.0001:\n",
    "        print(\"→ Excellent precision: well-calibrated system\")\n",
    "    elif g_precision < 0.0005:\n",
    "        print(\"→ Good precision: typical EPR spectrometer\")\n",
    "    elif g_precision < 0.001:\n",
    "        print(\"→ Moderate precision: check field stability\")\n",
    "    else:\n",
    "        print(\"→ Poor precision: significant anisotropy or calibration issues\")\n",
    "\n",
    "# Theoretical g-factor considerations\n",
    "print(\"\\n4. g-Factor Interpretation:\")\n",
    "if g_factors:\n",
    "    avg_g = np.mean(g_factors)\n",
    "    delta_g = avg_g - CONSTANTS['g_free']\n",
    "    \n",
    "    print(f\"Δg = g - g_e = {delta_g:+.6f}\")\n",
    "    \n",
    "    if abs(delta_g) < 0.001:\n",
    "        print(\"→ Orbital contribution: minimal (pure spin system)\")\n",
    "        print(\"→ Likely species: organic radicals, some transition metals\")\n",
    "    elif delta_g > 0.001:\n",
    "        print(\"→ Orbital contribution: positive (unquenched orbital moment)\")\n",
    "        print(\"→ Possible species: transition metals, heteroatom radicals\")\n",
    "    elif delta_g < -0.001:\n",
    "        print(\"→ Orbital contribution: negative (rare, check calibration)\")\n",
    "        print(\"→ Possible issues: field calibration, frequency drift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spectral Integration and Quantification\n",
    "\n",
    "Integration of EPR spectra provides quantitative information about spin concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single and double integration of EPR spectrum\n",
    "print(\"Performing spectral integration...\")\n",
    "\n",
    "# Single integration (first derivative → absorption)\n",
    "dx = x[1] - x[0]  # Field step\n",
    "absorption = np.cumsum(y) * dx\n",
    "\n",
    "# Remove baseline drift from integrated spectrum\n",
    "absorption_corrected = absorption - np.linspace(absorption[0], absorption[-1], len(absorption))\n",
    "\n",
    "# Double integration (absorption → double integral)\n",
    "double_integral = np.cumsum(absorption_corrected) * dx\n",
    "\n",
    "# Calculate total spin count (proportional to double integral area)\n",
    "total_double_integral = double_integral[-1] - double_integral[0]\n",
    "\n",
    "print(f\"Field step: {dx:.4f} G\")\n",
    "print(f\"Single integral range: {absorption_corrected.min():.3f} to {absorption_corrected.max():.3f}\")\n",
    "print(f\"Double integral total: {total_double_integral:.3e}\")\n",
    "\n",
    "# Estimate spin concentration (requires calibration)\n",
    "print(\"\\nSpin Quantification:\")\n",
    "print(\"Note: Absolute concentrations require calibration with known standards\")\n",
    "\n",
    "# Relative quantification based on double integral\n",
    "if abs(total_double_integral) > 0:\n",
    "    print(f\"Relative spin count: {abs(total_double_integral):.3e} arbitrary units\")\n",
    "    \n",
    "    # Estimate for typical EPR conditions (very rough)\n",
    "    # This would need proper calibration with known spin standards\n",
    "    typical_sensitivity = 1e12  # spins per unit double integral (very rough)\n",
    "    estimated_spins = abs(total_double_integral) * typical_sensitivity\n",
    "    \n",
    "    print(f\"Rough spin estimate: {estimated_spins:.1e} spins (requires calibration)\")\n",
    "    \n",
    "    # Convert to typical concentration units\n",
    "    sample_volume_ml = 0.1  # Typical EPR sample volume\n",
    "    avogadro = 6.022e23\n",
    "    \n",
    "    molar_concentration = (estimated_spins / sample_volume_ml) / (avogadro * 1000)\n",
    "    print(f\"Estimated concentration: {molar_concentration:.2e} M (very rough)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Zero total double integral - check spectrum baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize integration results\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Original first derivative spectrum\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=1.5, label='First derivative (original)')\n",
    "plt.xlabel('Magnetic Field (G)')\n",
    "plt.ylabel('dχ\"/dB (a.u.)')\n",
    "plt.title('EPR Spectrum - First Derivative')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Single integral (absorption)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x, absorption, 'gray', linewidth=1, alpha=0.7, label='Raw integral')\n",
    "plt.plot(x, absorption_corrected, 'r-', linewidth=2, label='Absorption (baseline corrected)')\n",
    "plt.xlabel('Magnetic Field (G)')\n",
    "plt.ylabel('χ\" (a.u.)')\n",
    "plt.title('Single Integration - Absorption Spectrum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Double integral\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x, double_integral, 'g-', linewidth=2, label='Double integral')\n",
    "plt.axhline(y=0, color='k', linestyle=':', alpha=0.5)\n",
    "plt.xlabel('Magnetic Field (G)')\n",
    "plt.ylabel('∫∫ Signal (a.u.)')\n",
    "plt.title('Double Integration - Proportional to Spin Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add integration statistics as text\n",
    "stats_text = f'Total Double Integral: {total_double_integral:.3e}\\n'\n",
    "stats_text += f'Peak Absorption: {np.max(np.abs(absorption_corrected)):.3f}\\n'\n",
    "stats_text += f'Absorption Width: {x[np.argmin(absorption_corrected)] - x[np.argmax(absorption_corrected)]:.1f} G'\n",
    "\n",
    "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "         verticalalignment='top', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Integration quality assessment\n",
    "print(\"\\nIntegration Quality Assessment:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check integration baseline\n",
    "baseline_drift = abs(double_integral[-1] - double_integral[0]) / len(x)\n",
    "signal_level = np.std(double_integral)\n",
    "\n",
    "print(f\"Double integral baseline drift: {baseline_drift:.2e} per point\")\n",
    "print(f\"Double integral signal level: {signal_level:.2e}\")\n",
    "\n",
    "if baseline_drift < signal_level * 0.01:\n",
    "    print(\"✓ Excellent integration: minimal baseline issues\")\n",
    "elif baseline_drift < signal_level * 0.1:\n",
    "    print(\"✓ Good integration: minor baseline correction needed\")\n",
    "else:\n",
    "    print(\"⚠ Integration issues: significant baseline problems\")\n",
    "\n",
    "# Signal-to-noise in integrated spectrum\n",
    "absorption_snr = np.max(np.abs(absorption_corrected)) / np.std(absorption_corrected[-50:])  # Noise from tail\n",
    "print(f\"\\nAbsorption spectrum S/N ratio: {absorption_snr:.1f}\")\n",
    "\n",
    "if absorption_snr > 50:\n",
    "    print(\"✓ Excellent S/N: reliable quantification\")\n",
    "elif absorption_snr > 20:\n",
    "    print(\"✓ Good S/N: quantification possible with care\")\n",
    "elif absorption_snr > 5:\n",
    "    print(\"⚠ Moderate S/N: quantification uncertain\")\n",
    "else:\n",
    "    print(\"✗ Poor S/N: quantification not reliable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperfine Structure Analysis\n",
    "\n",
    "Analysis of hyperfine splitting patterns provides information about nuclear-electron interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed hyperfine structure analysis\n",
    "print(\"Analyzing hyperfine structure...\")\n",
    "\n",
    "def analyze_hyperfine_pattern(positions, intensities=None):\n",
    "    \"\"\"Analyze hyperfine splitting pattern.\"\"\"\n",
    "    \n",
    "    if len(positions) < 2:\n",
    "        return {\"pattern\": \"single\", \"analysis\": \"No hyperfine splitting detected\"}\n",
    "    \n",
    "    # Calculate all splittings\n",
    "    splittings = []\n",
    "    for i in range(len(positions)-1):\n",
    "        split = positions[i+1] - positions[i]\n",
    "        splittings.append(split)\n",
    "    \n",
    "    # Analyze splitting pattern\n",
    "    avg_splitting = np.mean(splittings)\n",
    "    splitting_std = np.std(splittings)\n",
    "    \n",
    "    analysis = {\n",
    "        \"n_lines\": len(positions),\n",
    "        \"splittings\": splittings,\n",
    "        \"avg_splitting\": avg_splitting,\n",
    "        \"splitting_std\": splitting_std,\n",
    "        \"total_width\": positions[-1] - positions[0]\n",
    "    }\n",
    "    \n",
    "    # Pattern recognition\n",
    "    if len(positions) == 2:\n",
    "        analysis[\"pattern\"] = \"doublet\"\n",
    "        analysis[\"interpretation\"] = \"I=1/2 nucleus or two equivalent nuclei\"\n",
    "        analysis[\"coupling_constant\"] = avg_splitting\n",
    "        \n",
    "    elif len(positions) == 3:\n",
    "        # Check for equal spacing (I=1) vs unequal (overlapping patterns)\n",
    "        if splitting_std < avg_splitting * 0.1:  # Equal spacing within 10%\n",
    "            analysis[\"pattern\"] = \"triplet\"\n",
    "            analysis[\"interpretation\"] = \"I=1 nucleus (e.g., ¹⁴N) or three equivalent I=1/2 nuclei\"\n",
    "            analysis[\"coupling_constant\"] = avg_splitting\n",
    "        else:\n",
    "            analysis[\"pattern\"] = \"triplet_unequal\"\n",
    "            analysis[\"interpretation\"] = \"Overlapping hyperfine patterns or unequal couplings\"\n",
    "    \n",
    "    elif len(positions) == 4:\n",
    "        analysis[\"pattern\"] = \"quartet\"\n",
    "        analysis[\"interpretation\"] = \"I=3/2 nucleus or overlapping patterns\"\n",
    "    \n",
    "    elif len(positions) == 5:\n",
    "        analysis[\"pattern\"] = \"quintet\"\n",
    "        analysis[\"interpretation\"] = \"I=2 nucleus or four equivalent I=1/2 nuclei\"\n",
    "    \n",
    "    else:\n",
    "        analysis[\"pattern\"] = \"complex\"\n",
    "        analysis[\"interpretation\"] = \"Complex hyperfine structure - multiple nuclei\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze the detected pattern\n",
    "if zero_crossings:\n",
    "    hf_analysis = analyze_hyperfine_pattern(zero_crossings)\n",
    "    \n",
    "    print(f\"Hyperfine Pattern Analysis:\")\n",
    "    print(f\"Number of lines: {hf_analysis['n_lines']}\")\n",
    "    print(f\"Pattern type: {hf_analysis['pattern']}\")\n",
    "    print(f\"Interpretation: {hf_analysis['interpretation']}\")\n",
    "    \n",
    "    if 'coupling_constant' in hf_analysis:\n",
    "        A = hf_analysis['coupling_constant']\n",
    "        print(f\"\\nHyperfine coupling constant:\")\n",
    "        print(f\"A = {A:.3f} G = {A * 2.8:.3f} MHz\")\n",
    "        \n",
    "        # Compare with typical values\n",
    "        typical_couplings = {\n",
    "            \"¹⁴N nitroxide\": (14, 18, \"G\"),\n",
    "            \"¹H proton\": (0.5, 5, \"G\"),\n",
    "            \"¹³C carbon\": (5, 50, \"G\"),\n",
    "            \"³¹P phosphorus\": (10, 100, \"G\")\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nComparison with typical values:\")\n",
    "        for nucleus, (min_val, max_val, unit) in typical_couplings.items():\n",
    "            if min_val <= A <= max_val:\n",
    "                print(f\"  ✓ Consistent with {nucleus}: {min_val}-{max_val} {unit}\")\n",
    "            else:\n",
    "                print(f\"    {nucleus}: {min_val}-{max_val} {unit}\")\n",
    "    \n",
    "    print(f\"\\nDetailed splitting analysis:\")\n",
    "    for i, split in enumerate(hf_analysis['splittings']):\n",
    "        print(f\"  Δ{i+1}-{i+2}: {split:.3f} G\")\n",
    "    \n",
    "    if len(hf_analysis['splittings']) > 1:\n",
    "        print(f\"  Average: {hf_analysis['avg_splitting']:.3f} ± {hf_analysis['splitting_std']:.3f} G\")\n",
    "        print(f\"  Total width: {hf_analysis['total_width']:.3f} G\")\n",
    "        \n",
    "        # Check splitting regularity\n",
    "        regularity = hf_analysis['splitting_std'] / hf_analysis['avg_splitting']\n",
    "        if regularity < 0.05:\n",
    "            print(f\"  → Very regular spacing: single nucleus hyperfine\")\n",
    "        elif regularity < 0.15:\n",
    "            print(f\"  → Reasonably regular: likely single nucleus with some distortion\")\n",
    "        else:\n",
    "            print(f\"  → Irregular spacing: multiple nuclei or overlapping patterns\")\n",
    "\n",
    "else:\n",
    "    print(\"No hyperfine structure detected - single line spectrum\")\n",
    "    print(\"Possible causes:\")\n",
    "    print(\"  - No magnetic nuclei in radical\")\n",
    "    print(\"  - Very broad lines (short T₂)\")\n",
    "    print(\"  - Unresolved hyperfine structure\")\n",
    "    print(\"  - Fast exchange processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced hyperfine analysis and simulation\n",
    "if zero_crossings and len(zero_crossings) >= 2:\n",
    "    print(\"\\nAdvanced Hyperfine Analysis:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Intensity analysis (if we have peak heights)\n",
    "    peak_intensities = []\n",
    "    for pos in zero_crossings:\n",
    "        # Find closest point to zero crossing\n",
    "        idx = np.argmin(np.abs(x - pos))\n",
    "        # Look for nearby extrema as intensity measure\n",
    "        window = 20  # points around zero crossing\n",
    "        start_idx = max(0, idx - window)\n",
    "        end_idx = min(len(y), idx + window)\n",
    "        \n",
    "        # Peak-to-peak intensity around zero crossing\n",
    "        local_max = np.max(y[start_idx:end_idx])\n",
    "        local_min = np.min(y[start_idx:end_idx])\n",
    "        intensity = local_max - local_min\n",
    "        peak_intensities.append(intensity)\n",
    "    \n",
    "    print(f\"Relative intensities:\")\n",
    "    intensities_norm = np.array(peak_intensities) / np.max(peak_intensities)\n",
    "    for i, (pos, intensity) in enumerate(zip(zero_crossings, intensities_norm)):\n",
    "        print(f\"  Peak {i+1} ({pos:.1f} G): {intensity:.3f}\")\n",
    "    \n",
    "    # Compare with theoretical patterns\n",
    "    theoretical_patterns = {\n",
    "        \"I=1/2 (doublet)\": [1, 1],\n",
    "        \"I=1 (triplet)\": [1, 2, 1],\n",
    "        \"I=3/2 (quartet)\": [1, 3, 3, 1],\n",
    "        \"I=2 (quintet)\": [1, 4, 6, 4, 1],\n",
    "        \"3×I=1/2 (quartet)\": [1, 3, 3, 1],\n",
    "        \"4×I=1/2 (quintet)\": [1, 4, 6, 4, 1]\n",
    "    }\n",
    "    \n",
    "    n_lines = len(zero_crossings)\n",
    "    print(f\"\\nIntensity pattern matching:\")\n",
    "    \n",
    "    for pattern_name, expected_intensities in theoretical_patterns.items():\n",
    "        if len(expected_intensities) == n_lines:\n",
    "            # Normalize expected pattern\n",
    "            expected_norm = np.array(expected_intensities) / np.max(expected_intensities)\n",
    "            \n",
    "            # Calculate match quality (RMS difference)\n",
    "            rms_diff = np.sqrt(np.mean((intensities_norm - expected_norm)**2))\n",
    "            \n",
    "            match_quality = \"★★★\" if rms_diff < 0.1 else \"★★\" if rms_diff < 0.2 else \"★\" if rms_diff < 0.4 else \"\"\n",
    "            \n",
    "            print(f\"  {pattern_name:<20}: RMS diff = {rms_diff:.3f} {match_quality}\")\n",
    "    \n",
    "    # Linewidth analysis\n",
    "    print(f\"\\nLinewidth Analysis:\")\n",
    "    linewidths = []\n",
    "    \n",
    "    for i, pos in enumerate(zero_crossings):\n",
    "        # Find zero crossing index\n",
    "        idx = np.argmin(np.abs(x - pos))\n",
    "        \n",
    "        # Find peak-to-peak width around this zero crossing\n",
    "        window = 50  # points to search for extrema\n",
    "        start_idx = max(0, idx - window)\n",
    "        end_idx = min(len(y), idx + window)\n",
    "        \n",
    "        # Find local maximum and minimum\n",
    "        local_data = y[start_idx:end_idx]\n",
    "        local_x = x[start_idx:end_idx]\n",
    "        \n",
    "        max_idx = np.argmax(local_data) + start_idx\n",
    "        min_idx = np.argmin(local_data) + start_idx\n",
    "        \n",
    "        # Peak-to-peak linewidth\n",
    "        linewidth = abs(x[max_idx] - x[min_idx])\n",
    "        linewidths.append(linewidth)\n",
    "        \n",
    "        print(f\"  Line {i+1}: {linewidth:.2f} G\")\n",
    "    \n",
    "    if linewidths:\n",
    "        avg_linewidth = np.mean(linewidths)\n",
    "        linewidth_std = np.std(linewidths)\n",
    "        \n",
    "        print(f\"  Average: {avg_linewidth:.2f} ± {linewidth_std:.2f} G\")\n",
    "        \n",
    "        # Interpret linewidth\n",
    "        if avg_linewidth < 0.5:\n",
    "            print(f\"  → Very narrow lines: long T₂, solution spectrum\")\n",
    "        elif avg_linewidth < 2.0:\n",
    "            print(f\"  → Narrow lines: good resolution, moderate mobility\")\n",
    "        elif avg_linewidth < 5.0:\n",
    "            print(f\"  → Moderate linewidth: typical powder/frozen solution\")\n",
    "        else:\n",
    "            print(f\"  → Broad lines: short T₂, restricted motion\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Advanced EPR Analysis Complete\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Export\n",
    "\n",
    "Let's summarize all the analysis results and prepare them for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive analysis results\n",
    "print(\"Compiling Analysis Results...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "analysis_results = {\n",
    "    \"measurement_parameters\": {\n",
    "        \"frequency_ghz\": freq_ghz,\n",
    "        \"field_range_g\": [float(x.min()), float(x.max())],\n",
    "        \"data_points\": int(len(x)),\n",
    "        \"field_resolution_g\": float(x[1] - x[0]),\n",
    "        \"data_source\": data_source\n",
    "    },\n",
    "    \"peak_detection\": {\n",
    "        \"n_peaks\": len(zero_crossings) if zero_crossings else 0,\n",
    "        \"peak_positions_g\": [float(pos) for pos in zero_crossings] if zero_crossings else [],\n",
    "        \"hyperfine_splittings_g\": [float(s) for s in np.diff(zero_crossings)] if len(zero_crossings) > 1 else []\n",
    "    },\n",
    "    \"g_factors\": {\n",
    "        \"individual_g_values\": [float(g) for g in g_factors] if g_factors else [],\n",
    "        \"mean_g_factor\": float(np.mean(g_factors)) if g_factors else None,\n",
    "        \"g_factor_std\": float(np.std(g_factors)) if len(g_factors) > 1 else None,\n",
    "        \"delta_g_from_free_electron\": float(np.mean(g_factors) - CONSTANTS['g_free']) if g_factors else None\n",
    "    },\n",
    "    \"integration\": {\n",
    "        \"double_integral_total\": float(total_double_integral) if 'total_double_integral' in locals() else None,\n",
    "        \"absorption_peak_height\": float(np.max(np.abs(absorption_corrected))) if 'absorption_corrected' in locals() else None,\n",
    "        \"absorption_snr\": float(absorption_snr) if 'absorption_snr' in locals() else None\n",
    "    },\n",
    "    \"spectral_quality\": {\n",
    "        \"signal_to_noise\": float(np.ptp(y) / np.std(y)),\n",
    "        \"peak_to_peak_amplitude\": float(np.ptp(y)),\n",
    "        \"rms_amplitude\": float(np.sqrt(np.mean(y**2)))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add hyperfine analysis if available\n",
    "if 'hf_analysis' in locals():\n",
    "    analysis_results[\"hyperfine_analysis\"] = {\n",
    "        \"pattern_type\": hf_analysis['pattern'],\n",
    "        \"interpretation\": hf_analysis['interpretation'],\n",
    "        \"coupling_constant_g\": float(hf_analysis.get('coupling_constant', 0)),\n",
    "        \"coupling_constant_mhz\": float(hf_analysis.get('coupling_constant', 0) * 2.8),\n",
    "        \"total_splitting_width_g\": float(hf_analysis.get('total_width', 0))\n",
    "    }\n",
    "\n",
    "# Print formatted summary\n",
    "print(\"\\nEPR ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📊 MEASUREMENT PARAMETERS:\")\n",
    "print(f\"   Microwave frequency: {analysis_results['measurement_parameters']['frequency_ghz']:.3f} GHz\")\n",
    "print(f\"   Field range: {analysis_results['measurement_parameters']['field_range_g'][0]:.1f} - {analysis_results['measurement_parameters']['field_range_g'][1]:.1f} G\")\n",
    "print(f\"   Resolution: {analysis_results['measurement_parameters']['field_resolution_g']:.4f} G/point\")\n",
    "print(f\"   Data points: {analysis_results['measurement_parameters']['data_points']:,}\")\n",
    "\n",
    "print(f\"\\n🎯 PEAK ANALYSIS:\")\n",
    "n_peaks = analysis_results['peak_detection']['n_peaks']\n",
    "if n_peaks > 0:\n",
    "    print(f\"   Number of absorption peaks: {n_peaks}\")\n",
    "    for i, pos in enumerate(analysis_results['peak_detection']['peak_positions_g']):\n",
    "        print(f\"   Peak {i+1}: {pos:.2f} G\")\n",
    "    \n",
    "    if len(analysis_results['peak_detection']['hyperfine_splittings_g']) > 0:\n",
    "        avg_split = np.mean(analysis_results['peak_detection']['hyperfine_splittings_g'])\n",
    "        print(f\"   Average splitting: {avg_split:.2f} G\")\nelse:\n    print(f\"   No distinct peaks detected\")\n",
    "\n",
    "print(f\"\\n⚛️  G-FACTOR ANALYSIS:\")\n",
    "if analysis_results['g_factors']['mean_g_factor']:\n",
    "    mean_g = analysis_results['g_factors']['mean_g_factor']\n",
    "    delta_g = analysis_results['g_factors']['delta_g_from_free_electron']\n",
    "    print(f\"   Mean g-factor: {mean_g:.6f}\")\n",
    "    print(f\"   Δg (g - g_e): {delta_g:+.6f}\")\n",
    "    \n",
    "    if analysis_results['g_factors']['g_factor_std']:\n",
    "        g_std = analysis_results['g_factors']['g_factor_std']\n",
    "        print(f\"   g-factor spread: ±{g_std:.6f}\")\n",
    "else:\n",
    "    print(f\"   g-factor calculation not available\")\n",
    "\n",
    "print(f\"\\n📈 QUANTITATIVE ANALYSIS:\")\n",
    "if analysis_results['integration']['double_integral_total']:\n",
    "    di_total = analysis_results['integration']['double_integral_total']\n",
    "    print(f\"   Double integral: {di_total:.3e} (∝ spin count)\")\n",
    "    \n",
    "    if analysis_results['integration']['absorption_snr']:\n",
    "        abs_snr = analysis_results['integration']['absorption_snr']\n",
    "        print(f\"   Absorption S/N: {abs_snr:.1f}\")\nelse:\n    print(f\"   Quantitative analysis not performed\")\n",
    "\n",
    "print(f\"\\n🔍 SPECTRAL QUALITY:\")\n",
    "snr = analysis_results['spectral_quality']['signal_to_noise']\n",
    "ptp = analysis_results['spectral_quality']['peak_to_peak_amplitude']\n",
    "print(f\"   Signal-to-noise ratio: {snr:.1f}\")\n",
    "print(f\"   Peak-to-peak amplitude: {ptp:.3f}\")\n",
    "\n",
    "if 'hyperfine_analysis' in analysis_results:\n",
    "    print(f\"\\n🧬 HYPERFINE STRUCTURE:\")\n",
    "    hf = analysis_results['hyperfine_analysis']\n",
    "    print(f\"   Pattern: {hf['pattern_type']}\")\n",
    "    print(f\"   Interpretation: {hf['interpretation']}\")\n",
    "    if hf['coupling_constant_g'] > 0:\n",
    "        print(f\"   Coupling constant: {hf['coupling_constant_g']:.3f} G = {hf['coupling_constant_mhz']:.1f} MHz\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add metadata\n",
    "analysis_results[\"metadata\"] = {\n",
    "    \"analysis_date\": datetime.now().isoformat(),\n",
    "    \"epyr_tools_version\": \"development\",\n",
    "    \"analysis_type\": \"advanced_epr_analysis\",\n",
    "    \"data_source\": data_source\n",
    "}\n",
    "\n",
    "# Export as JSON\n",
    "json_output = output_dir / \"advanced_analysis_results.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Analysis results exported to: {json_output}\")\n",
    "\n",
    "# Export processed data\n",
    "import pandas as pd\n",
    "\n",
    "# Create comprehensive data export\n",
    "export_data = {\n",
    "    'field_gauss': x,\n",
    "    'epr_signal': y,\n",
    "    'absorption': absorption_corrected if 'absorption_corrected' in locals() else np.zeros_like(x),\n",
    "    'double_integral': double_integral if 'double_integral' in locals() else np.zeros_like(x)\n",
    "}\n",
    "\n",
    "df_export = pd.DataFrame(export_data)\n",
    "\n",
    "# Add analysis summary as header comments\n",
    "csv_output = output_dir / \"advanced_analysis_data.csv\"\n",
    "with open(csv_output, 'w') as f:\n",
    "    f.write(f\"# EPyR Tools - Advanced EPR Analysis Results\\n\")\n",
    "    f.write(f\"# Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"# Data source: {data_source}\\n\")\n",
    "    f.write(f\"# Frequency: {freq_ghz:.6f} GHz\\n\")\n",
    "    \n",
    "    if analysis_results['g_factors']['mean_g_factor']:\n",
    "        f.write(f\"# Mean g-factor: {analysis_results['g_factors']['mean_g_factor']:.6f}\\n\")\n",
    "    \n",
    "    if analysis_results['peak_detection']['n_peaks'] > 0:\n",
    "        f.write(f\"# Number of peaks: {analysis_results['peak_detection']['n_peaks']}\\n\")\n",
    "    \n",
    "    if analysis_results['integration']['double_integral_total']:\n",
    "        f.write(f\"# Double integral: {analysis_results['integration']['double_integral_total']:.3e}\\n\")\n",
    "    \n",
    "    f.write(f\"#\\n\")\n",
    "\n",
    "df_export.to_csv(csv_output, mode='a', index=False)\n",
    "print(f\"Processed data exported to: {csv_output}\")\n",
    "\n",
    "# Create summary plot for export\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Four-panel summary plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=1.5)\n",
    "if zero_crossings:\n",
    "    for pos in zero_crossings:\n",
    "        plt.axvline(x=pos, color='r', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Magnetic Field (G)')\n",
    "plt.ylabel('EPR Signal (a.u.)')\n",
    "plt.title('EPR Spectrum with Peak Positions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "if 'absorption_corrected' in locals():\n",
    "    plt.plot(x, absorption_corrected, 'r-', linewidth=1.5)\n",
    "    plt.xlabel('Magnetic Field (G)')\n",
    "    plt.ylabel('Absorption (a.u.)')\n",
    "    plt.title('Integrated Absorption Spectrum')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "if 'double_integral' in locals():\n",
    "    plt.plot(x, double_integral, 'g-', linewidth=1.5)\n",
    "    plt.xlabel('Magnetic Field (G)')\n",
    "    plt.ylabel('Double Integral (a.u.)')\n",
    "    plt.title('Double Integration (∝ Spin Count)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Analysis summary as text\n",
    "plt.axis('off')\n",
    "summary_text = f\"EPR Analysis Summary\\n\\n\"\n",
    "summary_text += f\"Frequency: {freq_ghz:.3f} GHz\\n\"\n",
    "\n",
    "if analysis_results['g_factors']['mean_g_factor']:\n",
    "    summary_text += f\"g-factor: {analysis_results['g_factors']['mean_g_factor']:.6f}\\n\"\n",
    "\n",
    "summary_text += f\"Peaks: {analysis_results['peak_detection']['n_peaks']}\\n\"\n",
    "\n",
    "if len(analysis_results['peak_detection']['hyperfine_splittings_g']) > 0:\n",
    "    avg_split = np.mean(analysis_results['peak_detection']['hyperfine_splittings_g'])\n",
    "    summary_text += f\"Avg. splitting: {avg_split:.2f} G\\n\"\n",
    "\n",
    "summary_text += f\"S/N ratio: {snr:.1f}\\n\"\n",
    "summary_text += f\"\\nData source: {data_source}\\n\"\n",
    "summary_text += f\"Analysis: {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n",
    "\n",
    "plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes,\n",
    "         verticalalignment='top', fontsize=12, fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.suptitle(f'EPyR Tools - Advanced EPR Analysis Summary', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save summary plot\n",
    "plot_output = output_dir / \"advanced_analysis_summary.png\"\n",
    "plt.savefig(plot_output, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Analysis summary plot saved: {plot_output}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"🎉 ADVANCED EPR ANALYSIS COMPLETE!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"📋 {json_output.name} - Complete analysis results (JSON)\")\n",
    "print(f\"📊 {csv_output.name} - Processed spectral data (CSV)\")\n",
    "print(f\"📈 {plot_output.name} - Analysis summary plot (PNG)\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"• Load results in other software for further analysis\")\n",
    "print(f\"• Compare with literature values or theoretical predictions\")\n",
    "print(f\"• Process additional spectra with Batch Processing tutorial\")\n",
    "print(f\"• Explore temperature-dependent or kinetic EPR experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Summary\n",
    "\n",
    "This advanced analysis tutorial demonstrated comprehensive EPR spectroscopy analysis using EPyR Tools:\n",
    "\n",
    "### ✨ Key Accomplishments\n",
    "\n",
    "1. **Peak Detection**: Automatic identification of absorption peaks and derivative extrema\n",
    "2. **g-Factor Analysis**: Precise calculation and interpretation of electronic g-factors\n",
    "3. **Hyperfine Analysis**: Pattern recognition and coupling constant determination\n",
    "4. **Quantitative Integration**: Single and double integration for spin quantification\n",
    "5. **Quality Assessment**: Signal-to-noise, linewidth, and precision analysis\n",
    "6. **Data Export**: Comprehensive results in JSON, CSV, and graphical formats\n",
    "\n",
    "### 🧪 EPR Physics Covered\n",
    "\n",
    "- **Electronic g-factors**: Orbital contributions and molecular environment effects\n",
    "- **Hyperfine interactions**: Nuclear-electron coupling and pattern interpretation\n",
    "- **Spectral integration**: Quantitative spin counting and concentration determination\n",
    "- **Linewidth analysis**: Relaxation processes and molecular dynamics\n",
    "- **Calibration methods**: Field stability and measurement precision\n",
    "\n",
    "### 📊 Analysis Capabilities\n",
    "\n",
    "- Handles synthetic and real EPR data automatically\n",
    "- Recognizes common hyperfine patterns (doublet, triplet, quartet, etc.)\n",
    "- Compares with literature values and theoretical predictions\n",
    "- Provides quality metrics for reliable quantification\n",
    "- Exports results in multiple formats for further analysis\n",
    "\n",
    "### 🔧 Best Practices Demonstrated\n",
    "\n",
    "1. **Data Quality**: Always assess S/N ratio and baseline correction\n",
    "2. **Peak Detection**: Use appropriate thresholds and validate results visually\n",
    "3. **g-Factor Precision**: Check frequency calibration and field stability\n",
    "4. **Integration**: Verify baseline correction before quantitative analysis\n",
    "5. **Pattern Recognition**: Compare experimental with theoretical intensity ratios\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "- **04_Batch_Processing.ipynb**: Process multiple EPR files efficiently\n",
    "- **Specialized Analysis**: Temperature dependence, kinetics, 2D spectroscopy\n",
    "- **Literature Comparison**: Match your results with published EPR parameters\n",
    "- **Method Development**: Adapt analysis workflows for your specific research\n",
    "\n",
    "The exported analysis results are ready for:\n",
    "- Publication in scientific papers\n",
    "- Comparison with computational predictions\n",
    "- Integration with other analytical techniques\n",
    "- Long-term data archival and FAIR data sharing\n",
    "\n",
    "**Happy EPR analyzing! 🎯⚛️📊**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"
  },\n",
   "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.9.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}
